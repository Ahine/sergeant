---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```
<!--
[![Build Status](https://travis-ci.org/hrbrmstr/sergeant.svg)](https://travis-ci.org/hrbrmstr/sergeant) 
![Project Status: Concept - Minimal or no implementation has been done yet.](http://www.repostatus.org/badges/0.1.0/concept.svg)](http://www.repostatus.org/#concept)
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/sergeant)](http://cran.r-project.org/web/packages/sergeant) 
![downloads](http://cranlogs.r-pkg.org/badges/grand-total/sergeant)
-->

<img src="sergeant.png" width="33" align="left" style="padding-right:20px"/>

`sergeant` : Tools to Transform and Query Data with the 'Apache Drill' 'REST API' and JDBC Interface

Drill + `sergeant` is (IMO) a nice alternative to Spark + `sparklyr` if you don't need the ML components of Spark (i.e. just need to query "big data" sources, need to interface with parquet, need to combine disperate data source types â€” json, csv, parquet, rdbms - for aggregation, etc). Drill also has support for spatial queries.

The package doesn't have a `dplyr`-esque interface yet, but creating one is possible since Drill uses pretty standard SQL for queries. Right now, you need to build Drill SQL queries by hand and issue them with `drill_query()`. It's good to get one's hands dirty with some SQL on occassion (it builds character). The JDBC interface may make it possible to write a `dplyr` wrapper for it. 

I find writing SQL queries to parquet files with Drill on a local 64GB Linux workstation to be more performant than doing the data ingestion work with R (for large or disperate data sets). I also work with many tiny JSON files on a daily basis and Drill makes it much easier to do so. YMMV.

You can download Drill from <https://drill.apache.org/download/> (use "Direct File Download"). I use `/usr/local/drill` as the install directory. `drill-embedded` is a super-easy way to get started playing with Drill on a single workstation and most of my workflows can get by using Drill this way. If there is sufficient desire for an automated downloader and a way to start the `drill-embedded` server from within R, please file an issue.

There are a few convenience wrappers for various informational SQL queries (like `drill_version()`). Please file an PR if you add more.

The package has been written with retrieval of rectangular data sources in mind. If you need/want a version of `drill_query()` that will enable returning of non-rectangular data (which is possible with Drill) then please file an issue.

Some of the more "controlling vs data ops" REST API functions aren't implemented. Please file a PR if you need those.

Finally, I run most of this locally and at home, so it's all been coded with no authentication or encryption in mind. If you want/need support for that, please file an issue. If there is demand for this, it will change the R API a bit (I've already thought out what to do but have no need for it right now).

The following functions are implemented:

- `drill_connection`: Setup parameters for a Drill server/cluster connection
- `drill_active`: Test whether Drill HTTP REST API server is up
- `drill_cancel`:	Cancel the query that has the given queryid
- `drill_jdbc`:	Connect to Drill using JDBC _(driver included with package until CRAN release)_
- `drill_metrics`:	Get the current memory metrics
- `drill_options`:	List the name, default, and data type of the system and session options
- `drill_profile`:	Get the profile of the query that has the given query id
- `drill_profiles`:	Get the profiles of running and completed queries
- `drill_query`:	Submit a query and return results
- `drill_set`:	Set Drill SYSTEM or SESSION options
- `drill_settings_reset`:	Changes (optionally, all) session settings back to system defaults
- `drill_show_files`:	Show files in a file system schema.
- `drill_show_schemas`:	Returns a list of available schemas.
- `drill_stats`:	Get Drillbit information, such as ports numbers
- `drill_status`:	Get the status of Drill
- `drill_storage`:	Get the list of storage plugin names and configurations
- `drill_system_reset`:	Changes (optionally, all) system settings back to system defaults
- `drill_threads`:	Get information about threads
- `drill_uplift`:	Turn a columnar query results into a type-converted tbl
- `drill_use`:	Change to a particular schema.
- `drill_version`:	Identify the version of Drill running

### Installation

```{r eval=FALSE}
devtools::install_github("hrbrmstr/sergeant")
```

```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
options(width=120)
```

### Experimental `dplyr` interface

```{r message=FALSE}
library(sergeant)
library(dplyr)

ds <- src_drill("localhost:31010", use_zk=FALSE)

ds

db <- tbl(ds, "cp.`employee.json`") 

count(db, gender, marital_status)

group_by(db, position_title) %>% 
  count(gender) %>% 
  mutate(full_desc=ifelse(gender=="F", "Female", "Male"))
```

### Usage

```{r}
library(sergeant)

# current verison
packageVersion("sergeant")

dc <- drill_connection("localhost") 

drill_active(dc)

drill_version(dc)

drill_storage(dc)$name
```

Working with the built-in JSON data sets:

```{r}
drill_query(dc, "SELECT * FROM cp.`employee.json` limit 100")

drill_query(dc, "SELECT COUNT(gender) AS gender FROM cp.`employee.json` GROUP BY gender")

drill_options(dc)

drill_options(dc, "json")
```

## Working with parquet files

```{r}
drill_query(dc, "SELECT * FROM dfs.`/usr/local/drill/sample-data/nation.parquet` LIMIT 5")
```

Including multiple parquet files in different directories (note the wildcard support):

```{r}
drill_query(dc, "SELECT * FROM dfs.`/usr/local/drill/sample-data/nations*/nations*.parquet` LIMIT 5")
```

### A preview of the built-in support for spatial ops

Via: <https://github.com/k255/drill-gis>

A common use case is to select data within boundary of given polygon:

```{r}
drill_query(dc, "
select columns[2] as city, columns[4] as lon, columns[3] as lat
    from cp.`sample-data/CA-cities.csv`
    where
        ST_Within(
            ST_Point(columns[4], columns[3]),
            ST_GeomFromText(
                'POLYGON((-121.95 37.28, -121.94 37.35, -121.84 37.35, -121.84 37.28, -121.95 37.28))'
                )
            )
")
```

### JDBC

```{r}
library(RJDBC)

con <- drill_jdbc("localhost:31010", use_zk=FALSE)

drill_query(con, "SELECT * FROM cp.`employee.json`")

# but it can work via JDBC function calls, too
dbGetQuery(con, "SELECT * FROM cp.`employee.json`") %>% 
  tibble::as_tibble()
```

### Use in knitr SQL code chunks

If you install `knit` via GitHub (`devtools::install_github("yihui/knitr")) you can use the `sql` chunk code type with `drill_jdbc()` connections:

    ---
    output: html_document
    ---

    ```{r libraries, message=FALSE}
    library(sergeant)
    library(DBI)
    library(RJDBC)
    ```

    ## Setup JDBC connection

    ```{r conn_setup}
    dc <- drill_jdbc("localhost:31010", use_zk=FALSE)
    ```

    ## Test out a query

    ```{sql, connection=dc}
    SELECT * FROM cp.`employee.json`
    ```

Which is (IMO) _way_ better than using the Drill consoles, the Drill Web UI query box or SQLWorkbench.

### Test Results

```{r}
library(sergeant)
library(testthat)

date()

test_dir("tests/")
```

### Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). 
By participating in this project you agree to abide by its terms.
